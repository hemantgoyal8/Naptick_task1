# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zDSU2TQ9wYWHpWlQ8TrspjeKIzBNS1YS
"""

# Step 0: Mount Drive and Install Libraries
from google.colab import drive
drive.mount('/content/drive')

!pip install langchain python-dotenv sentence-transformers faiss-cpu gradio tiktoken -q
!pip install accelerate bitsandbytes einops -q
!pip install transformers torch -q # Install transformers with PyTorch support
!pip install --upgrade langchain langchain-community jq -q # Added jq here
print("Required libraries installed.")

# Step 1: Project Setup and Hugging Face Login
import os
import huggingface_hub
from google.colab import userdata

# Define the main project path in your Google Drive
project_path = '/content/drive/MyDrive/Naptick_Challenge' # You can change 'Naptick_Challenge' if you prefer
data_base_path = os.path.join(project_path, 'data')
vector_store_base_path = os.path.join(project_path, 'vector_stores')

# Create base directory and subdirectories
os.makedirs(project_path, exist_ok=True)
os.makedirs(data_base_path, exist_ok=True)
os.makedirs(vector_store_base_path, exist_ok=True)

# Create data subfolders
collections_dirs = ['wearable', 'chat_history', 'user_profile', 'location', 'custom_collection']
for collection_dir_name in collections_dirs:
    os.makedirs(os.path.join(data_base_path, collection_dir_name), exist_ok=True)

print(f"Project folder structure created/ensured at: {project_path}")

# Hugging Face Login
try:
    hf_token = userdata.get('HF_TOKEN')
    huggingface_hub.login(token=hf_token, add_to_git_credential=True)
    print("Successfully logged into Hugging Face Hub!")
except userdata.SecretNotFoundError:
    print("HF_TOKEN secret not found in Colab secrets.")
except Exception as e:
    print(f"An error occurred during Hugging Face login: {e}")

# Library Version Checks and GPU Check
import transformers
import torch
import langchain

print(f"Transformers version: {transformers.__version__}")
print(f"Torch version: {torch.__version__}")
print(f"LangChain version: {langchain.__version__}")

if torch.cuda.is_available():
    print(f"GPU detected: {torch.cuda.get_device_name(0)}")
else:
    print("WARNING: No GPU detected. Model loading might be very slow or fail.")

# Step 2a: Create Sample Wearable Data (Generalized)
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

wearable_data_path = os.path.join(data_base_path, 'wearable', 'sample_wearable_data.csv')
data = []
start_date = datetime.now() - timedelta(days=10)

for day_offset in range(10):
    current_date_base = start_date + timedelta(days=day_offset)
    sleep_onset_hour = np.random.randint(22, 24)
    sleep_onset_minute = np.random.randint(0, 59)
    sleep_duration_hours = np.random.normal(loc=7.5, scale=0.75)
    sleep_duration_hours = max(5, min(9.5, sleep_duration_hours))
    sleep_time = current_date_base.replace(hour=sleep_onset_hour, minute=sleep_onset_minute, second=0, microsecond=0)
    wake_time = sleep_time + timedelta(hours=sleep_duration_hours)
    total_sleep_mins = int(sleep_duration_hours * 60)
    deep_sleep_percentage = max(0.05, min(0.30, np.random.normal(loc=0.18, scale=0.05)))
    rem_sleep_percentage = max(0.10, min(0.35, np.random.normal(loc=0.22, scale=0.05)))
    light_sleep_percentage = max(0, 1 - deep_sleep_percentage - rem_sleep_percentage)
    deep_sleep_mins = int(total_sleep_mins * deep_sleep_percentage)
    rem_sleep_mins = int(total_sleep_mins * rem_sleep_percentage)
    light_sleep_mins = total_sleep_mins - deep_sleep_mins - rem_sleep_mins
    awakenings = np.random.randint(0, 5)
    sleep_score = np.random.randint(60, 95)

    data.append([sleep_time.strftime('%Y-%m-%d %H:%M:%S'), 'WHOOP', 'sleep_onset_time', sleep_time.strftime('%H:%M')])
    data.append([wake_time.strftime('%Y-%m-%d %H:%M:%S'), 'WHOOP', 'wake_up_time', wake_time.strftime('%H:%M')])
    data.append([wake_time.strftime('%Y-%m-%d %H:%M:%S'), 'WHOOP', 'total_sleep_duration_mins', total_sleep_mins])
    data.append([wake_time.strftime('%Y-%m-%d %H:%M:%S'), 'WHOOP', 'deep_sleep_duration_mins', deep_sleep_mins])
    data.append([wake_time.strftime('%Y-%m-%d %H:%M:%S'), 'WHOOP', 'rem_sleep_duration_mins', rem_sleep_mins])
    data.append([wake_time.strftime('%Y-%m-%d %H:%M:%S'), 'WHOOP', 'light_sleep_duration_mins', light_sleep_mins])
    data.append([wake_time.strftime('%Y-%m-%d %H:%M:%S'), 'WHOOP', 'sleep_awakenings_count', awakenings])
    data.append([wake_time.strftime('%Y-%m-%d %H:%M:%S'), 'WHOOP', 'sleep_score_percent', sleep_score])

    steps_taken = np.random.randint(3000, 15000)
    active_minutes = np.random.randint(20, 120)
    calories_burned = 1800 + steps_taken // 20 + active_minutes * 5
    data.append([current_date_base.replace(hour=23, minute=59).strftime('%Y-%m-%d %H:%M:%S'), 'Fitbit', 'steps_count', steps_taken])
    data.append([current_date_base.replace(hour=23, minute=59).strftime('%Y-%m-%d %H:%M:%S'), 'Fitbit', 'active_minutes_total', active_minutes])
    data.append([current_date_base.replace(hour=23, minute=59).strftime('%Y-%m-%d %H:%M:%S'), 'Fitbit', 'calories_burned_total', calories_burned])

    for hr_sample_hour in range(6, 23, 2):
        hr_time = current_date_base.replace(hour=hr_sample_hour, minute=np.random.randint(0,59))
        hr = np.random.randint(50, 90) # Simplified
        data.append([hr_time.strftime('%Y-%m-%d %H:%M:%S'), 'Apple Health', 'heart_rate_bpm', hr])
    rhr_time = current_date_base.replace(hour=5, minute=np.random.randint(0,59))
    rhr = np.random.randint(45, 65)
    data.append([rhr_time.strftime('%Y-%m-%d %H:%M:%S'), 'Apple Health', 'resting_heart_rate_bpm', rhr])

df_wearable = pd.DataFrame(data, columns=['timestamp', 'source', 'metric', 'value']) # No user_id
df_wearable.to_csv(wearable_data_path, index=False)
print(f"Saved generalized wearable data to: {wearable_data_path} ({len(df_wearable)} points)")
print(df_wearable.head())

# Step 2b: Create Sample Chat History (Generalized)
import json
chat_history_path = os.path.join(data_base_path, 'chat_history', 'sample_chat_history.json')
chat_log = [
    {'timestamp': (datetime.now() - timedelta(days=5, hours=1)).strftime('%Y-%m-%d %H:%M:%S'), 'speaker': 'user', 'text': 'How did I sleep last night?'},
    {'timestamp': (datetime.now() - timedelta(days=5, hours=1, minutes=-1)).strftime('%Y-%m-%d %H:%M:%S'), 'speaker': 'bot', 'text': 'Looking at your WHOOP data from last night, you slept for 7 hours and 45 minutes. This included 85 minutes of deep sleep and 105 minutes of REM. Your sleep score was 78%.'},
    {'timestamp': (datetime.now() - timedelta(days=3, hours=2)).strftime('%Y-%m-%d %H:%M:%S'), 'speaker': 'user', 'text': 'I felt a bit tired. What were my steps yesterday?'},
    {'timestamp': (datetime.now() - timedelta(days=3, hours=2, minutes=-1)).strftime('%Y-%m-%d %H:%M:%S'), 'speaker': 'bot', 'text': 'Yesterday, your Fitbit data shows you took 8,250 steps and had 65 active minutes.'},
] # No user_id
with open(chat_history_path, 'w') as f:
    json.dump(chat_log, f, indent=4)
print(f"Saved generalized chat history to: {chat_history_path}")
with open(chat_history_path, 'r') as f: print(json.dumps(json.load(f), indent=2))

# Step 2c: Create Sample User Profile Data (Generalized)
user_profile_dir = os.path.join(data_base_path, 'user_profile')
main_profile_path = os.path.join(user_profile_dir, 'main_user_profile.json') # Generic filename
user_profile_content = {
    "name_alias": "Alice", # Can still use a name for addressing
    "age_group_approx": "30s",
    "sleep_preference_time_general": "Around 10:30 PM - 6:30 AM",
    "primary_device_sleep_preference": "WHOOP",
    "primary_device_activity_preference": "Fitbit",
    "stated_goals": ["improve REM sleep", "feel more energetic consistently", "understand impact of diet on sleep"],
    "general_preferences": ["concise answers but with key details", "actionable advice", "interested in workout impact on sleep"],
    "general_notes": "User sometimes reports feeling groggy even after 7+ hours of sleep. Interested in optimizing sleep stages."
} # No user_id field
with open(main_profile_path, 'w') as f:
    json.dump(user_profile_content, f, indent=4)
print(f"Saved generalized user profile to: {main_profile_path}")
# Remove old user_1_profile.json if it exists
old_profile_path = os.path.join(user_profile_dir, 'user_1_profile.json')
if os.path.exists(old_profile_path):
    os.remove(old_profile_path)
    print(f"Removed old profile file: {old_profile_path}")


# Step 2d: Create Sample Location Data (Generalized)
location_data_path = os.path.join(data_base_path, 'location', 'sample_location_data.csv')
location_data = [
    [(datetime.now() - timedelta(days=8)).strftime('%Y-%m-%d %H:%M:%S'), 34.0522, -118.2437, 'Home base - Los Angeles area'],
    [(datetime.now() - timedelta(days=7, hours=9)).strftime('%Y-%m-%d %H:%M:%S'), 34.0114, -118.4949, 'Workplace - Santa Monica office area'],
    [(datetime.now() - timedelta(days=3, hours=10)).strftime('%Y-%m-%d %H:%M:%S'), 33.9416, -118.4085, 'Travel event - LAX Airport area'],
    [(datetime.now() - timedelta(days=2, hours=14)).strftime('%Y-%m-%d %H:%M:%S'), 40.7128, -74.0060, 'Visit - NYC Hotel area'],
] # No user_id
df_location = pd.DataFrame(location_data, columns=['timestamp', 'latitude', 'longitude', 'place_description'])
df_location.to_csv(location_data_path, index=False)
print(f"Saved generalized location data to: {location_data_path}")
print(df_location.head())

# Step 2e: Create Sample Custom Collection Data (Unchanged, already general)
custom_collection_dir = os.path.join(data_base_path, 'custom_collection')
# Content from your original script, which is good and general
sleep_hygiene_content = "Comprehensive Guide to Sleep Hygiene: 1. Maintain a Consistent Sleep Schedule... 2. Create a Relaxing Bedtime Ritual..." # (Shortened for brevity here)
deep_sleep_content = "Importance of Deep Sleep (Slow-Wave Sleep): Deep sleep is crucial for physical restoration..."
caffeine_content = "Caffeine and Sleep: Caffeine is a stimulant that blocks adenosine receptors..."
rem_sleep_content = "Understanding REM Sleep: REM (Rapid Eye Movement) sleep is a unique phase of sleep..."
sleep_stages_content = "The Stages of Human Sleep: Human sleep is not a monolithic state but progresses through several distinct stages..."

# (Full content from your script should be used here)
# For brevity, I'm using placeholders. Ensure you paste the full content from your original script.
full_sleep_hygiene_content = """
Comprehensive Guide to Sleep Hygiene:
1.  **Maintain a Consistent Sleep Schedule:** Go to bed and wake up around the same time every day, including weekends. This helps regulate your body's internal clock (circadian rhythm). Aim for 7-9 hours of sleep per night.
2.  **Create a Relaxing Bedtime Ritual:** Engage in calming activities for 30-60 minutes before bed. Examples: Reading a physical book, taking a warm bath, listening to calming music, meditation, journaling.
3.  **Optimize Your Sleep Environment:** Dark, quiet, cool (60-67°F or 15-19°C), comfortable.
4.  **Limit Exposure to Blue Light Before Bed:** Avoid smartphones, tablets, computers, TVs for at least an hour before bedtime.
5.  **Watch Your Diet and Fluid Intake:** Avoid large meals, caffeine, alcohol close to bedtime. Limit evening fluids.
6.  **Get Regular Physical Activity:** Daily exercise improves sleep, but avoid vigorous workouts 1-2 hours before bed.
7.  **Manage Naps Wisely:** Short naps (20-30 mins) early in the afternoon are best.
8.  **Use Your Bed Only for Sleep and Intimacy:** Avoid working, eating, or watching TV in bed.
9.  **Address Stress and Worry:** Try relaxation techniques or journaling.
10. **Limit Nicotine:** Nicotine is a stimulant.
"""
with open(os.path.join(custom_collection_dir, 'sleep_hygiene_detailed.txt'), 'w') as f: f.write(full_sleep_hygiene_content)
with open(os.path.join(custom_collection_dir, 'deep_sleep_info.txt'), 'w') as f: f.write(deep_sleep_content)
with open(os.path.join(custom_collection_dir, 'caffeine_effects.txt'), 'w') as f: f.write(caffeine_content)
with open(os.path.join(custom_collection_dir, 'rem_sleep_info.txt'), 'w') as f: f.write(rem_sleep_content)
with open(os.path.join(custom_collection_dir, 'sleep_stages_explained.txt'), 'w') as f: f.write(sleep_stages_content)

print(f"Saved custom collection files to: {custom_collection_dir}")
print(os.listdir(custom_collection_dir))


# Step 3: Load Data from Collections (Generalized)
from langchain_community.document_loaders import DirectoryLoader, TextLoader, JSONLoader # Removed CSVLoader from here for manual handling
from langchain.docstore.document import Document
import pandas as pd # ADDED
import json

# --- MODIFIED: Load Wearable Data Manually ---
df_wearable_loaded = pd.read_csv(wearable_data_path)
wearable_docs_raw = []
for index, row in df_wearable_loaded.iterrows():
    page_content = (
        f"Wearable data event at {row.get('timestamp', 'N/A')}. "
        f"Source device: {row.get('source', 'N/A')}. "
        f"Metric: {row.get('metric', 'N/A')}, Value: {row.get('value', 'N/A')}."
    )
    metadata = row.to_dict() # Keep all original columns as metadata
    metadata['collection'] = 'wearable'
    metadata['original_source_file'] = os.path.basename(wearable_data_path) # Add original file
    wearable_docs_raw.append(Document(page_content=page_content, metadata=metadata))
print(f"Loaded {len(wearable_docs_raw)} docs from Wearable. Sample: {wearable_docs_raw[0].page_content[:150]}... Meta: {wearable_docs_raw[0].metadata}")

# Load Chat History (Your manual loading for chat is good)
chat_docs = []
with open(chat_history_path, 'r') as f: chat_log_loaded = json.load(f)
for message in chat_log_loaded:
    doc = Document(
        page_content=f"{message.get('speaker', 'unknown')}: {message.get('text', '')}",
        metadata={'timestamp': message.get('timestamp', 'N/A'),
                  'speaker': message.get('speaker', 'unknown'),
                  'collection': 'chat_history',
                  'original_source_file': os.path.basename(chat_history_path)
                  }
    )
    chat_docs.append(doc)
print(f"\nLoaded {len(chat_docs)} docs from Chat History. Sample: {chat_docs[0].page_content} Meta: {chat_docs[0].metadata}")

# Load User Profile (DirectoryLoader for JSON is generally fine, but ensure content is correct)
# Ensure your main_user_profile.json DOES NOT contain "user_id" keys.
# If it does, delete the file from Drive and rerun Step 2c.
def extract_simplified_profile_metadata(metadata_param: dict, content: dict = None) -> dict:
    metadata_param['collection'] = 'user_profile'
    # metadata_param['original_source_file'] = metadata_param.get('source') # Already has 'source'
    return metadata_param

profile_loader = DirectoryLoader(
    user_profile_dir, glob="*.json", loader_cls=JSONLoader,
    loader_kwargs={'jq_schema': '.', 'json_lines': False, 'metadata_func': extract_simplified_profile_metadata, 'text_content': False},
    show_progress=True
)
profile_docs_raw = profile_loader.load()
for doc in profile_docs_raw:
    doc.page_content = json.dumps(doc.page_content) # Convert dict content to string
    doc.metadata['original_source_file'] = doc.metadata.get('source') # Store the full path if needed
print(f"\nLoaded {len(profile_docs_raw)} docs from User Profile. Sample: {profile_docs_raw[0].page_content[:100]}... Meta: {profile_docs_raw[0].metadata}")


# --- MODIFIED: Load Location Data Manually ---
df_location_loaded = pd.read_csv(location_data_path)
location_docs_raw = []
for index, row in df_location_loaded.iterrows():
    page_content = (
        f"A travel or location event was recorded around {row.get('timestamp', 'N/A')}. "
        f"The place was noted as: '{row.get('place_description', 'an undescribed place')}' "
        f"(Coordinates: Lat {row.get('latitude', 'N/A')}, Lon {row.get('longitude', 'N/A')})."
    )
    metadata = row.to_dict()
    metadata['collection'] = 'location'
    metadata['original_source_file'] = os.path.basename(location_data_path)
    location_docs_raw.append(Document(page_content=page_content, metadata=metadata))
print(f"\nLoaded {len(location_docs_raw)} docs from Location. Sample: {location_docs_raw[0].page_content[:150]}... Meta: {location_docs_raw[0].metadata}")

# Load Custom Collection (TextLoader is good)
custom_loader = DirectoryLoader(custom_collection_dir, glob="*.txt", loader_cls=TextLoader, show_progress=True)
custom_docs_raw = custom_loader.load()
for doc in custom_docs_raw:
    doc.metadata['collection'] = 'custom_collection'
    # doc.metadata['original_source_file'] = doc.metadata.get('source') # Already has 'source'
print(f"\nLoaded {len(custom_docs_raw)} docs from Custom Collection. Sample: {custom_docs_raw[0].page_content[:100]}... Meta: {custom_docs_raw[0].metadata}")

docs_by_collection = {
    "wearable": wearable_docs_raw, "chat_history": chat_docs,
    "user_profile": profile_docs_raw, "location": location_docs_raw,
    "custom_collection": custom_docs_raw
}
total_docs_loaded = sum(len(docs) for docs in docs_by_collection.values())
print(f"\n--- Total documents loaded across all collections: {total_docs_loaded} ---")


# Step 4: Chunk Documents (Primarily for Custom Collection)
from langchain.text_splitter import RecursiveCharacterTextSplitter
import time

chunk_size = 512
chunk_overlap = 50
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size, chunk_overlap=chunk_overlap,
    length_function=len, add_start_index=True
)
print(f"Initialized Text Splitter: chunk_size={chunk_size}, overlap={chunk_overlap}")

# Chunk only the custom_collection as it's text-heavy
docs_to_chunk = docs_by_collection['custom_collection']
start_time = time.time()
chunked_custom_docs = text_splitter.split_documents(docs_to_chunk)
print(f"Chunking 'custom_collection' took {time.time() - start_time:.2f}s. Original: {len(docs_to_chunk)}, Chunked: {len(chunked_custom_docs)}")

final_docs_for_embedding = []
final_docs_for_embedding.extend(docs_by_collection['wearable'])
final_docs_for_embedding.extend(docs_by_collection['chat_history'])
final_docs_for_embedding.extend(docs_by_collection['user_profile'])
final_docs_for_embedding.extend(docs_by_collection['location'])
final_docs_for_embedding.extend(chunked_custom_docs) # Add chunked custom docs

print(f"\n--- Total documents prepared for embedding: {len(final_docs_for_embedding)} ---")


# Step 5: Initialize Embedding Model
from langchain_community.embeddings import HuggingFaceEmbeddings

model_name = "sentence-transformers/all-MiniLM-L6-v2"
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model_kwargs = {'device': device}
encode_kwargs = {'normalize_embeddings': True}
print(f"Configured embedding model: {model_name} on device: {device}")

try:
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs
    )
    print("Successfully loaded embedding model.")
    if device == 'cuda': torch.cuda.empty_cache()
    sample_vector = embeddings.embed_query("Test embedding.")
    print(f"Test embedding successful. Dimension: {len(sample_vector)}")
except Exception as e:
    print(f"Error loading embedding model: {e}")
    raise e


# Step 6: Create and Save/Load FAISS Vector Stores

import shutil
print("Attempting to delete old FAISS index directories...")
for collection_name_to_delete in docs_by_collection.keys():
    old_index_path = os.path.join(vector_store_base_path, f"{collection_name_to_delete}_faiss_index")
    if os.path.exists(old_index_path):
        try:
            shutil.rmtree(old_index_path)
            print(f"Deleted old index: {old_index_path}")
        except Exception as e:
            print(f"Error deleting {old_index_path}: {e}")
    else:
        print(f"Old index not found (good): {old_index_path}")
print("Deletion attempt complete.")

from langchain_community.vectorstores import FAISS
import pickle

index_paths = {
    coll: os.path.join(vector_store_base_path, f"{coll}_faiss_index")
    for coll in docs_by_collection.keys()
}
print("Defined FAISS index paths.")

created_vector_stores = {}
print("\n--- Starting Index Creation/Loading ---")

for collection_name, docs_list_original in docs_by_collection.items():
    print(f"\nProcessing collection: {collection_name}...")
    current_docs_for_indexing = chunked_custom_docs if collection_name == 'custom_collection' else docs_list_original

    if not current_docs_for_indexing:
        print(f"Skipping '{collection_name}', no documents.")
        continue

    index_path = index_paths[collection_name]
    if os.path.exists(index_path):
        print(f"Loading existing index for '{collection_name}' from {index_path}")
        try:
            start_time = time.time()
            vector_store = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
            created_vector_stores[collection_name] = vector_store
            print(f"Loaded in {time.time() - start_time:.2f}s.")
        except Exception as e:
            print(f"Error loading index for {collection_name}: {e}. Will try to recreate.")
            # Potentially delete corrupted index: shutil.rmtree(index_path)
            # For now, just fall through to recreate
    # Recreate if not exists or loading failed
    if collection_name not in created_vector_stores:
        print(f"Creating FAISS index for {len(current_docs_for_indexing)} documents...")
        start_time = time.time()
        try:
            vector_store = FAISS.from_documents(documents=current_docs_for_indexing, embedding=embeddings)
            print(f"Index creation took {time.time() - start_time:.2f}s.")
            vector_store.save_local(index_path)
            print(f"Index saved to: {index_path}")
            created_vector_stores[collection_name] = vector_store
        except Exception as e:
            print(f"!!! ERROR creating/saving index for {collection_name}: {e}")
            import traceback
            traceback.print_exc()

print("\n--- Index Creation/Loading Complete ---")
loaded_vector_stores = created_vector_stores # Use these for retrieval


# Step 7: Implement Retrieval Function (Generalized)
from langchain.schema import Document
import re
import traceback

if not loaded_vector_stores:
    print("CRITICAL WARNING: 'loaded_vector_stores' is empty. Retrieval will fail.")

# In Step 7: Implement Retrieval Function (Generalized)

# ... (imports) ...

def retrieve_context(query: str, k_per_store: int = 3) -> tuple[str, list[Document]]: # Default k
    request_id = time.strftime('%Y%m%d-%H%M%S')
    print(f"\n[{request_id}] === retrieve_context START (Generalized) ===")
    print(f"[{request_id}] Query: '{query}', k_per_store (default): {k_per_store}")

    all_retrieved_docs_before_dedup = []
    retrieval_start_time = time.time()

    if not loaded_vector_stores:
         print(f"[{request_id}] ERROR: loaded_vector_stores is empty!")
         return "Error: Vector stores not loaded.", []

    for collection_name, vector_store in loaded_vector_stores.items():
        current_k = k_per_store
        # --- MODIFICATION: Increase k for location data ---
        if collection_name == 'location':
            current_k = max(k_per_store, 5) # Fetch at least 5 docs from location, or more if k_per_store is higher
            print(f"[{request_id}] Using k={current_k} for 'location' collection.")
        # --- END MODIFICATION ---
        else:
            print(f"[{request_id}] Using k={current_k} for '{collection_name}' collection.")


        print(f"[{request_id}] Processing collection: '{collection_name}' with k={current_k}")
        try:
            current_collection_docs = vector_store.similarity_search(query, k=current_k)
            print(f"[{request_id}]   Retrieved {len(current_collection_docs)} documents from '{collection_name}'.")

            # --- ADDED: Log retrieved content from location for debugging ---
            if collection_name == 'location' and current_collection_docs:
                print(f"[{request_id}] DEBUG: Top retrieved from 'location':")
                for i, d in enumerate(current_collection_docs[:3]): # Log top 3
                    print(f"[{request_id}]   L{i}: {d.page_content[:100]}...")
            # --- END ADDED DEBUG LOG ---

            for doc_obj in current_collection_docs:
                if hasattr(doc_obj, 'metadata'):
                    doc_obj.metadata['collection_retrieved_from'] = collection_name
                else:
                    print(f"[{request_id}] WARNING: Doc from '{collection_name}' has no metadata.")
            all_retrieved_docs_before_dedup.extend(current_collection_docs)

        except Exception as e:
            print(f"[{request_id}] !!! ERROR searching collection '{collection_name}': {e}")
            traceback.print_exc()

    # ... (rest of the function: dedup, formatting context) ...
    # (Make sure the rest of your retrieve_context function is here)

    retrieval_duration = time.time() - retrieval_start_time
    print(f"[{request_id}] Retrieval took {retrieval_duration:.2f}s. Docs before dedup: {len(all_retrieved_docs_before_dedup)}")

    unique_docs_final = []
    seen_content_keys = set()
    for doc_obj in all_retrieved_docs_before_dedup:
        page_content_str = getattr(doc_obj, 'page_content', None)
        if not isinstance(page_content_str, str): continue
        content_key = ' '.join(page_content_str.split())
        if content_key and content_key not in seen_content_keys:
            unique_docs_final.append(doc_obj)
            seen_content_keys.add(content_key)
    print(f"[{request_id}] Total unique documents after de-duplication: {len(unique_docs_final)}")

    context_separator = "\n\n---\n\n"
    if not unique_docs_final:
        formatted_context_str = "No specific information found relevant to this query." # Default if nothing found
    else:
        context_pieces = []
        for i, doc_obj in enumerate(unique_docs_final): # Iterate through unique_docs_final
            coll_name = doc_obj.metadata.get('collection_retrieved_from', doc_obj.metadata.get('collection', 'Unknown'))
            content = getattr(doc_obj, 'page_content', 'Error: Content missing')
            source_file = doc_obj.metadata.get('source', 'N/A') # Get original source filename if available
            context_pieces.append(f"Source Collection: {coll_name} (File: {os.path.basename(source_file) if source_file != 'N/A' else 'N/A'})\nContent: {content}")
        formatted_context_str = context_separator.join(context_pieces)

    print(f"[{request_id}] Formatted context length: {len(formatted_context_str)}")
    # --- ADDED: Log the final context if it's short or specific to travel ---
    if "travel" in query.lower() or "location" in query.lower() or "where" in query.lower():
        print(f"[{request_id}] DEBUG: Final formatted context for travel/location query (first 500 chars):\n{formatted_context_str[:500]}")
    # --- END ADDED DEBUG LOG ---
    print(f"[{request_id}] === retrieve_context END ===\n")
    return formatted_context_str, unique_docs_final

# Test generalized retrieval
if __name__ == '__main__':
    print("\n--- Testing Generalized Retrieval ---")
    if loaded_vector_stores:
        test_q1 = "What are tips for sleep hygiene?"
        ctx1, docs1 = retrieve_context(test_q1, k_per_store=2)
        print(f"Context for '{test_q1}':\n{ctx1[:300]}...\nDocs: {len(docs1)}")
        for d in docs1[:2]: print(f"  - {d.metadata.get('collection_retrieved_from')}: {d.page_content[:50]}...")

        test_q2 = "How much deep sleep was recorded recently?" # Was user_1 specific
        ctx2, docs2 = retrieve_context(test_q2, k_per_store=3)
        print(f"\nContext for '{test_q2}':\n{ctx2[:300]}...\nDocs: {len(docs2)}")
        for d in docs2[:2]: print(f"  - {d.metadata.get('collection_retrieved_from')}: {d.page_content[:50]}...")

        test_q3 = "What are the stated goals in the profile?" # Was user_2 specific (now general)
        ctx3, docs3 = retrieve_context(test_q3, k_per_store=1)
        print(f"\nContext for '{test_q3}':\n{ctx3[:300]}...\nDocs: {len(docs3)}")
        if docs3: print(f"  - {docs3[0].metadata.get('collection_retrieved_from')}: {docs3[0].page_content[:50]}...")
    else:
        print("Skipping retrieval tests, vector stores not loaded.")


# Step 8: Initialize LLM and Prompt Template
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain.prompts import PromptTemplate
import warnings

warnings.filterwarnings("ignore", category=UserWarning, message=".*padding_mask.*")
warnings.filterwarnings("ignore", category=FutureWarning)

model_id = "mistralai/Mistral-7B-Instruct-v0.2" # Using Mistral as per your original code
print(f"Loading LLM: {model_id}")
bnb_config = None
if torch.cuda.is_available():
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True, bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16
    )
    print("Using 4-bit quantization.")
else:
    print("GPU not available. LLM loading will be slow/unquantized.")

try:
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config if torch.cuda.is_available() else None,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        if hasattr(model, 'config') and model.config is not None:
             model.config.pad_token_id = tokenizer.eos_token_id
    print("LLM and Tokenizer loaded.")
except Exception as e:
    print(f"Error loading LLM/Tokenizer: {e}")
    traceback.print_exc()
    raise e

text_gen_pipeline = pipeline(
    "text-generation", model=model, tokenizer=tokenizer,
    max_new_tokens=512, do_sample=True, temperature=0.6,
    top_p=0.9, top_k=50, repetition_penalty=1.1,
    pad_token_id=tokenizer.eos_token_id
)
llm_pipeline = HuggingFacePipeline(pipeline=text_gen_pipeline)
print("Text generation pipeline wrapped in LangChain.")

# Generalized Prompt
template = """<s>[INST] You are Naptick AI, a factual and helpful sleep and wellness assistant for {user_name}.
Your purpose is to answer questions based STRICTLY on the information in the 'Context' and 'Chat History'.
DO NOT invent information or topics not explicitly asked for.
If the question is about data (like sleep, activity, location), provide a direct answer using only that data from the 'Context'.
If the question is general (e.g., sleep tips), answer using only the general knowledge provided in the 'Context'.

Your response MUST directly address the 'User Question'.
Address the user as {user_name} if it feels natural.
Responses should be accurate and concise, pulling directly from provided text.

If the 'Context' and 'Chat History' DO NOT contain information to answer the question, respond with:
"I'm sorry, {user_name}, I couldn't find specific information in my knowledge base to answer your question: '{query}'. Please try rephrasing or asking about something else."
DO NOT attempt to answer if context is insufficient.

Context:
{context}

Chat History:
{chat_history}

User Question: {query} [/INST]

Assistant Answer:"""
prompt_template = PromptTemplate(
    input_variables=["context", "chat_history", "query", "user_name"],
    template=template
)
print("Generalized prompt template defined.")


# Step 9: Implement Memory Layer and RAG Chain (Generalized)
from langchain.memory import ConversationBufferMemory
from langchain.schema.runnable import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

conversation_memory = ConversationBufferMemory(
    memory_key="chat_history", input_key="query", return_messages=False
)
print("ConversationBufferMemory initialized.")

def prepare_chain_inputs(input_dict, k_per_store=3):
    query = input_dict['query']
    print(f"    [prepare_chain_inputs] Retrieving context for query: '{query[:50]}...' with k={k_per_store}")
    # No user-specific logic for retrieval, retrieve_context is now general
    formatted_context, _ = retrieve_context(query, k_per_store=k_per_store)

    memory_variables = conversation_memory.load_memory_variables({})
    chat_history_string = memory_variables.get('chat_history', '')

    user_name_for_prompt = "Alice" # Default persona name for addressing user
    print(f"    [prepare_chain_inputs] User name for prompt: {user_name_for_prompt}")
    print(f"    [prepare_chain_inputs] Context length: {len(formatted_context)}, History length: {len(chat_history_string)}")
    return {
        "query": query,
        "context": formatted_context,
        "chat_history": chat_history_string,
        "user_name": user_name_for_prompt # For addressing in prompt
    }

rag_chain = (
    RunnablePassthrough()
    | RunnablePassthrough.assign(prepared_inputs=prepare_chain_inputs)
    | (lambda x: x['prepared_inputs'])
    | prompt_template
    | llm_pipeline
    | StrOutputParser()
)
print("RAG Chain with LCEL created (Generalized).")

# Test chain and memory
if __name__ == '__main__':
    print("\n--- Testing Generalized Chain and Memory ---")
    try:
        test_q_chain1 = "What are general tips for good sleep hygiene?"
        response1 = rag_chain.invoke({"query": test_q_chain1})
        print(f"Q1: {test_q_chain1}\nA1: {response1}")
        conversation_memory.save_context({"query": test_q_chain1}, {"output": response1})

        test_q_chain2 = "What about caffeine and its effects?" # Follow-up
        response2 = rag_chain.invoke({"query": test_q_chain2})
        print(f"\nQ2: {test_q_chain2}\nA2: {response2}")
        conversation_memory.save_context({"query": test_q_chain2}, {"output": response2})

        print("\n--- Current Memory ---")
        print(conversation_memory.load_memory_variables({}))
    except Exception as e:
        print(f"Error during chain test: {e}")
        traceback.print_exc()


# Step 10: Gradio Interface (Using Generalized RAG)
import gradio as gr
from threading import Thread
from transformers import TextIteratorStreamer
import psutil # For memory logging

# Ensure objects from previous steps exist (sanity check)
if 'text_gen_pipeline' not in locals() or 'conversation_memory' not in locals() or \
   'prepare_chain_inputs' not in locals() or 'prompt_template' not in locals():
    raise NameError("One or more required components (pipeline, memory, etc.) not found. Re-run previous cells.")

def print_memory_usage(stage=""):
    try:
        process = psutil.Process(os.getpid())
        ram_gb = process.memory_info().rss / (1024**3)
        ts = time.strftime('%H:%M:%S')
        print(f"[{ts}] RAM ({stage}): {ram_gb:.2f} GB", end="")
        if torch.cuda.is_available():
            gpu_alloc_gb = torch.cuda.memory_allocated(0) / (1024**3)
            gpu_res_gb = torch.cuda.memory_reserved(0) / (1024**3)
            print(f" | GPU ({stage}): Alloc={gpu_alloc_gb:.2f}GB, Res={gpu_res_gb:.2f}GB")
        else:
            print() # Newline if no GPU
    except Exception as e:
        print(f"[{time.strftime('%H:%M:%S')}] Error getting memory usage: {e}")

conversation_memory.clear() # Clear for fresh Gradio start
print("Chat memory cleared for Gradio.")
print_memory_usage("Before Gradio Launch")

def stream_response_gradio(user_message):
    req_start_time = time.time()
    ts_start = time.strftime('%H:%M:%S')
    print(f"\n[{ts_start}] === Gradio Request: '{user_message[:50]}...' ===")
    print_memory_usage("Gradio Req Start")

    streamer, thread, full_response = None, None, ""
    stream_ok = False

    try:
        if not hasattr(text_gen_pipeline, 'tokenizer') or text_gen_pipeline.tokenizer is None:
             raise ValueError("Pipeline tokenizer invalid.")
        streamer = TextIteratorStreamer(text_gen_pipeline.tokenizer, skip_prompt=True, skip_special_tokens=True)

        prep_start_time = time.time()
        # Use k_per_store=2 or 3 for Gradio for a balance of info and speed
        inputs = prepare_chain_inputs({'query': user_message}, k_per_store=2) # k=2 for Gradio
        print(f"[{time.strftime('%H:%M:%S')}] Inputs prepared in {time.time() - prep_start_time:.2f}s.")
        print_memory_usage("After Input Prep (Gradio)")

        prompt = prompt_template.format(**inputs)

        generate_kwargs = dict(
            text_inputs=prompt, streamer=streamer, max_new_tokens=512, # Same as before
            do_sample=True, temperature=0.6, top_p=0.9, repetition_penalty=1.1,
            pad_token_id=text_gen_pipeline.tokenizer.eos_token_id
        )

        thread = Thread(target=text_gen_pipeline, kwargs=generate_kwargs)
        thread.start()
        thread_start_time = time.time()
        print(f"[{time.strftime('%H:%M:%S')}] Generation thread started.")

        first_token_time = None
        for new_text in streamer:
            if first_token_time is None:
                first_token_time = time.time()
                print(f"[{time.strftime('%H:%M:%S')}] First token after {first_token_time - thread_start_time:.2f}s.")
            yield new_text
            full_response += new_text

        stream_ok = True
        if first_token_time: print(f"\n[{time.strftime('%H:%M:%S')}] Streaming done in {time.time() - first_token_time:.2f}s (after 1st token).")
        else: print(f"\n[{time.strftime('%H:%M:%S')}] Streaming done (no tokens).")
        print_memory_usage("After Streaming (Gradio)")

    except Exception as e:
        print(f"[{time.strftime('%H:%M:%S')}] !! ERROR during Gradio streaming/setup: {e}")
        traceback.print_exc()
        yield "\n\n[System Error: Could not process request.]"
        print_memory_usage("After Stream/Setup Error (Gradio)")
        return

    if stream_ok:
        try:
            if thread: thread.join()

            processed_response = full_response
            for stop_phrase in ["\nUser:", "\nAssistant:", "\nHuman:", "[INST]", "[/INST]"]: # Added INST tags
                 hallucination_pos = processed_response.find(stop_phrase)
                 if hallucination_pos != -1:
                      print(f"[{time.strftime('%H:%M:%S')}] Truncating hallucinated turn: '{stop_phrase}'")
                      processed_response = processed_response[:hallucination_pos].strip()
                      break

            final_cleaned_response = processed_response.strip()
            if not final_cleaned_response:
                final_cleaned_response = "I apologize, I couldn't generate a valid response for that."

            conversation_memory.save_context({"query": user_message}, {"output": final_cleaned_response})
            print(f"[{time.strftime('%H:%M:%S')}] Context saved. Bot: '{final_cleaned_response[:50]}...'")
            print_memory_usage("After Saving Context (Gradio)")

        except Exception as e:
            print(f"[{time.strftime('%H:%M:%S')}] !! ERROR post-streaming (Gradio): {e}")
            traceback.print_exc()
            yield "\n\n[System Error: Could not finalize response.]"
            print_memory_usage("After Post-Stream Error (Gradio)")
            return

    print(f"[{time.strftime('%H:%M:%S')}] === Gradio Request End. Total: {time.time() - req_start_time:.2f}s ===")
    print_memory_usage("Gradio Req End")


def chat_interface_fn_gradio(user_message, history):
    print(f"\n>>> [{time.strftime('%H:%M:%S')}] Gradio UI called: {user_message}")
    response_gen = stream_response_gradio(user_message)
    buffer = ""
    for chunk in response_gen:
        buffer += chunk
        yield buffer
    print(f"<<< [{time.strftime('%H:%M:%S')}] Gradio UI finished for: {user_message}")


chat_app = gr.ChatInterface(
    fn=chat_interface_fn_gradio,
    title="Naptick AI Sleep Coach (Generalized RAG)",
    description="Ask questions about sleep, (simulated) health data, or general wellness. All data is treated as general knowledge.",
    examples=[
        "What are some tips for a better sleep?",
        "How much deep sleep was recorded recently?", # Will search all data
        "What does research say about caffeine and sleep?",
        "What are the main goals mentioned in the profile?", # Corrected typo from "profiler"
        "Any recent travel events logged?",
        "How did the user sleep last night according to the data?" # "user" is generic here
    ],
    chatbot=gr.Chatbot(height=550, label="Chat", show_label=False),
    textbox=gr.Textbox(placeholder="Ask your question...", container=False, scale=7),
    # retry_btn=None,  # Set to None to disable and avoid potential errors
    # undo_btn=None,   # Set to None to disable and avoid potential errors
    # clear_btn=None,  # Set to None to disable and avoid potential errors (you can try "Clear Conversation" if this works)
)

# --- Launch Gradio App ---
print("\nLaunching Gradio Interface (Generalized)...")
chat_app.launch(debug=True, share=True) # Share=True for public link if needed